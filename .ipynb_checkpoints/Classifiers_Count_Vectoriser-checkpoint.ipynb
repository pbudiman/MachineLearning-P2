{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "solar-maximum",
   "metadata": {},
   "source": [
    "## COUNT VECTORIZER - Real Train & Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-walker",
   "metadata": {},
   "source": [
    "### Raw Train & Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electric-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Test to see approximate accuracy by splitting the training data to train and test data = 7:3\n",
    "\n",
    "base_path = \"./COMP30027_2021_Project2_datasets/\"\n",
    "\n",
    "train_data = pd.read_csv(base_path+\"recipe_train.csv\")\n",
    "X_train = train_data.iloc[:,:-1]\n",
    "y_train = train_data.iloc[:,-1]\n",
    "\n",
    "test_data = pd.read_csv(base_path+\"recipe_test.csv\")\n",
    "X_test = test_data.iloc[:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-diving",
   "metadata": {},
   "source": [
    "### Count Vectoriser for text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comic-kidney",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.3 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import scipy \n",
    "\n",
    "count_vec_path = base_path+\"recipe_text_features_countvec/\"\n",
    "\n",
    "# Get the sparse matrix of the Bag-of-Word representation of text features for training data\n",
    "\n",
    "# TRAIN & TEST DATASET - NAME\n",
    "name_file = count_vec_path + \"train_name_countvectorizer.pkl\"\n",
    "vocab_name = pickle.load(open(name_file, \"rb\"))\n",
    "train_name_matrix = scipy.sparse.load_npz(count_vec_path +'train_name_vec.npz')\n",
    "test_name_matrix = scipy.sparse.load_npz(count_vec_path +'test_name_vec.npz')\n",
    "df_train_name = pd.DataFrame(train_name_matrix.todense(),columns = vocab_name.get_feature_names())\n",
    "df_test_name = pd.DataFrame(test_name_matrix.todense(),columns = vocab_name.get_feature_names())\n",
    "\n",
    "# TRAIN & TEST DATASET - STEPS\n",
    "steps_file = count_vec_path +\"train_steps_countvectorizer.pkl\"\n",
    "vocab_steps = pickle.load(open(steps_file, \"rb\"))\n",
    "train_steps_matrix = scipy.sparse.load_npz(count_vec_path +'train_steps_vec.npz')\n",
    "test_steps_matrix = scipy.sparse.load_npz(count_vec_path +'test_steps_vec.npz')\n",
    "df_train_steps = pd.DataFrame(train_steps_matrix.todense(),columns = vocab_steps.get_feature_names())\n",
    "df_test_steps = pd.DataFrame(test_steps_matrix.todense(),columns = vocab_steps.get_feature_names())\n",
    "\n",
    "# TRAIN & TEST DATASET- INGREDIENTS\n",
    "ingr_file = count_vec_path + \"train_ingr_countvectorizer.pkl\"\n",
    "vocab_ingr = pickle.load(open(ingr_file, \"rb\"))\n",
    "train_ingr_matrix = scipy.sparse.load_npz(count_vec_path +'train_ingr_vec.npz')\n",
    "test_ingr_matrix = scipy.sparse.load_npz(count_vec_path +'test_ingr_vec.npz')\n",
    "df_train_ingr = pd.DataFrame(train_ingr_matrix.todense(),columns = vocab_ingr.get_feature_names())\n",
    "df_test_ingr = pd.DataFrame(test_ingr_matrix.todense(),columns = vocab_ingr.get_feature_names())\n",
    "\n",
    "# TRAIN & TEST DATASET- N_STEPS\n",
    "train_n_steps = pd.DataFrame(X_train.n_steps)\n",
    "train_n_steps.reset_index(drop=True, inplace=True)\n",
    "test_n_steps = pd.DataFrame(X_test.n_steps)\n",
    "test_n_steps.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# TRAIN & TEST DATASET- N_INGREDIENTS\n",
    "train_n_ingredients = pd.DataFrame(X_train.n_ingredients)\n",
    "train_n_ingredients.reset_index(drop=True, inplace=True)\n",
    "test_n_ingredients = pd.DataFrame(X_test.n_ingredients)\n",
    "test_n_ingredients.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# STILL USING ALL FEATURES AND THEIR MATRICES\n",
    "train = pd.concat([df_train_name,df_train_steps,df_train_ingr,train_n_steps,train_n_ingredients],axis=1)\n",
    "test = pd.concat([df_test_name,df_test_steps,df_test_ingr,test_n_steps,test_n_ingredients],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STACKING FROM W8 Prac\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)     \n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-advocacy",
   "metadata": {},
   "source": [
    "### Individual Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using each individual classifiers\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from time import ctime\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          svm.LinearSVC(),\n",
    "          DecisionTreeClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'LinearSVC',\n",
    "          'Decision Tree',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    print(title)\n",
    "    start = time.time()\n",
    "    print(\"START \",start)\n",
    "    model.fit(X_train,y_train)\n",
    "    result = model.predict(X_test)\n",
    "    end = time.time()\n",
    "    print(\"END \",end)\n",
    "    t = end - start\n",
    "    print(title,'Time:', t,\" s\")\n",
    "    df_res_full = pd.DataFrame(result, columns = ['duration_label'])\n",
    "    df_res_full.index = df_res_full.index + 1\n",
    "    df_res_full.index.name='id'\n",
    "    df_res_full.to_csv('df_CV_res_'+title+'_full.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using Stacking \n",
    "# Base Classifiers : Multinomial Naive Bayes + Decision Tree\n",
    "# Meta Classifier : Logistic Regression\n",
    "classifiers1 = [MultinomialNB(),\n",
    "                DecisionTreeClassifier()]\n",
    "\n",
    "titles1 = ['MNB',\n",
    "           'Decision Tree']\n",
    "\n",
    "meta_classifier_lr1 = LogisticRegression()\n",
    "stacker_lr1 = StackingClassifier(classifiers1, meta_classifier_lr1)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Meta learner: Logistic Regression - Start\",start)\n",
    "stacker_lr1.fit(X_train, y_train)\n",
    "stacker_lr1_res = stacker_lr1.predict(X_test)\n",
    "end = time.time()\n",
    "print(\"Meta learner: Logistic Regression - End \",end)\n",
    "t = end - start\n",
    "print('Time:', t,\" s\")\n",
    "df_res = pd.DataFrame(stacker_lr1_res, columns = ['duration_label'])\n",
    "df_res.index = df_res.index + 1\n",
    "df_res.index.name='id'\n",
    "df_res.to_csv('df_CV_stack1_Log_Reg.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "traditional-circulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  1621402145.123966\n",
      "end  1621402217.3950982\n",
      "time:  72.27113223075867\n"
     ]
    }
   ],
   "source": [
    "# KBEST -chi2\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import time\n",
    "from time import ctime\n",
    "start= time.time()\n",
    "print(\"start \",start)\n",
    "kbest_chi2 = SelectKBest(chi2, k=1000).fit(train, y_train)\n",
    "X_train_kbest_chi2 = kbest_chi2.transform(train)\n",
    "X_test_kbest_chi2 = kbest_chi2.transform(test)\n",
    "# print(X_train_kbest_chi2.shape)\n",
    "# print(X_test_kbest_chi2.shape)\n",
    "end= time.time()\n",
    "print(\"end \",end)\n",
    "print(\"time: \",end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pregnant-palestine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB\n",
      "START  1621402253.2543108\n",
      "END  1621402253.6907241\n",
      "MNB Time: 0.4364132881164551  s\n",
      "LinearSVC\n",
      "START  1621402253.770417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END  1621402280.263106\n",
      "LinearSVC Time: 26.49268913269043  s\n",
      "Decision Tree\n",
      "START  1621402280.296611\n",
      "END  1621402289.7775822\n",
      "Decision Tree Time: 9.480971097946167  s\n",
      "Logistic Regression\n",
      "START  1621402289.8047051\n",
      "END  1621402299.612122\n",
      "Logistic Regression Time: 9.807416915893555  s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from time import ctime\n",
    "models = [MultinomialNB(),\n",
    "          svm.LinearSVC(),\n",
    "          DecisionTreeClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['MNB',\n",
    "          'LinearSVC',\n",
    "          'Decision Tree',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    print(title)\n",
    "    start = time.time()\n",
    "    print(\"START \",start)\n",
    "    model.fit(X_train_kbest_chi2,y_train)\n",
    "    result = model.predict(X_test_kbest_chi2)\n",
    "    end = time.time()\n",
    "    print(\"END \",end)\n",
    "    t = end - start\n",
    "    print(title,'Time:', t,\" s\")\n",
    "    df_res_full = pd.DataFrame(result, columns = ['duration_label'])\n",
    "    df_res_full.index = df_res_full.index + 1\n",
    "    df_res_full.index.name='id'\n",
    "    df_res_full.to_csv('df_CV_res_chi2_'+title+'_full.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fitted-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from time import ctime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gross-universe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1 STACKING META LEARNER - COUNT VEC: LOGISTIC REG , BASE: MNB, DT \n",
      "Base Learners: MNB & DT\n",
      "MNB\n",
      "START  1621402361.190933\n",
      "END  1621402361.4258718\n",
      "MNB Time: 0.2349388599395752  s\n",
      "Decision Tree\n",
      "START  1621402361.454274\n",
      "END  1621402371.034938\n",
      "Decision Tree Time: 9.580664157867432  s\n",
      "START meta learner: Logistic Regression  1621402371.061424\n",
      "END meta learner: Logistic Regression  1621402588.660233\n",
      "Time: 217.59880900382996  s\n"
     ]
    }
   ],
   "source": [
    "print(\"#1 STACKING META LEARNER - COUNT VEC: LOGISTIC REG , BASE: MNB, DT \")\n",
    "classifiers1 = [MultinomialNB(),\n",
    "          DecisionTreeClassifier()]\n",
    "titles1 = ['MNB',\n",
    "          'Decision Tree']\n",
    "\n",
    "meta_classifier_lr1 = LogisticRegression()\n",
    "stacker_lr1 = StackingClassifier(classifiers1, meta_classifier_lr1)\n",
    "\n",
    "print(\"Base Learners: MNB & DT\")\n",
    "for title, clf in zip(titles1, classifiers1):\n",
    "    print(title)\n",
    "    start = time.time()\n",
    "    print(\"START \",start)\n",
    "    clf.fit(X_train_kbest_chi2,y_train)\n",
    "    result = clf.predict(X_test_kbest_chi2)\n",
    "    end = time.time()\n",
    "    print(\"END \",end)\n",
    "    t = end - start\n",
    "    print(title,'Time:', t,\" s\")\n",
    "    df_res_full = pd.DataFrame(result, columns = ['duration_label'])\n",
    "    df_res_full.index = df_res_full.index + 1\n",
    "    df_res_full.index.name='id'\n",
    "    df_res_full.to_csv('df_CV_chi2_base_1_'+title+'.csv')\n",
    "\n",
    "start = time.time()\n",
    "print(\"START meta learner: Logistic Regression \",start)\n",
    "stacker_lr1.fit(train, y_train)\n",
    "stacker_lr1_res= stacker_lr1.predict(test)\n",
    "end = time.time()\n",
    "print(\"END meta learner: Logistic Regression \",end)\n",
    "t = end - start\n",
    "print('Time:', t,\" s\")\n",
    "df_res = pd.DataFrame(stacker_lr1_res, columns = ['duration_label'])\n",
    "df_res.index = df_res.index + 1\n",
    "df_res.index.name='id'\n",
    "df_res.to_csv('df_CV_chi2_stack1_Log_Reg.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-gibraltar",
   "metadata": {},
   "source": [
    "# KBEST - F_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "industrial-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  1621398619.578366\n",
      "end  1621398789.440448\n",
      "time:  169.86208200454712\n"
     ]
    }
   ],
   "source": [
    "#MUTUAL INFORMATION\n",
    "# from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "# from matplotlib import pyplot\n",
    "\n",
    "# mi = SelectKBest(score_func=mutual_info_classif, k=1000)\n",
    "# print(\"FITTING\")\n",
    "# X_train_mi = mi.fit_transform(df_train_name,y_train)\n",
    "    \n",
    "# for feat_num in mi.get_support(indices=True):\n",
    "#     print(vocab_name.get_feature_names()[feat_num])\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2,f_classif\n",
    "import time\n",
    "from time import ctime\n",
    "start= time.time()\n",
    "print(\"start \",start)\n",
    "X_new = SelectKBest(f_classif, k=1000).fit_transform(train, y_train)\n",
    "X_new.shape\n",
    "end= time.time()\n",
    "print(\"end \",end)\n",
    "print(\"time: \",end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "overhead-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Test to see approximate accuracy by splitting the training data to train and test data = 7:3\n",
    "\n",
    "data = pd.read_csv(\"recipe_train.csv\")\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X,y,test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "promotional-death",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26800, 2686) (13200, 2686)\n",
      "(26800, 26361)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Test to see approximate accuracy by splitting the training data to train and test data = 7:3\n",
    "\n",
    "data = pd.read_csv(\"recipe_train.csv\")\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X,y,test_size=0.33)\n",
    "\n",
    "train_n_steps = pd.DataFrame(X_train_raw.n_steps)\n",
    "train_n_steps.reset_index(drop=True, inplace=True)\n",
    "test_n_steps = pd.DataFrame(X_test_raw.n_steps)\n",
    "test_n_steps.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_n_ingredients = pd.DataFrame(X_train_raw.n_ingredients)\n",
    "train_n_ingredients.reset_index(drop=True, inplace=True)\n",
    "test_n_ingredients = pd.DataFrame(X_test_raw.n_ingredients)\n",
    "test_n_ingredients.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# count vectorizer on feature 'name'\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "X_train_name = vec.fit_transform(X_train_raw.name)\n",
    "X_test_name = vec.transform(X_test_raw.name)\n",
    "df_train_name = pd.DataFrame(X_train_name.todense(),columns = vec.get_feature_names())\n",
    "df_test_name = pd.DataFrame(X_test_name.todense(),columns = vec.get_feature_names())\n",
    "\n",
    "# count vectorizer on feature 'steps'\n",
    "X_train_steps = vec.fit_transform(X_train_raw.steps)\n",
    "X_test_steps = vec.transform(X_test_raw.steps)\n",
    "df_train_steps = pd.DataFrame(X_train_steps.todense(),columns = vec.get_feature_names())\n",
    "df_test_steps = pd.DataFrame(X_test_steps.todense(),columns = vec.get_feature_names())\n",
    "\n",
    "\n",
    "# count vectorizer on feature 'ingredients'\n",
    "X_train_ing = vec.fit_transform(X_train_raw.ingredients)\n",
    "X_test_ing = vec.transform(X_test_raw.ingredients)\n",
    "df_train_ing = pd.DataFrame(X_train_ing.todense(),columns = vec.get_feature_names())\n",
    "df_test_ing = pd.DataFrame(X_test_ing.todense(),columns = vec.get_feature_names())\n",
    "print(X_train_ing.shape, X_test_ing.shape)\n",
    "\n",
    "# put all into one dataframe\n",
    "train = pd.concat([df_train_name, df_train_steps,df_train_ing,train_n_steps,train_n_ingredients],axis=1)\n",
    "test = pd.concat([df_test_name, df_test_steps,df_test_ing,test_n_steps,test_n_ingredients],axis=1)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "connected-concord",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  1621403008.1928792\n",
      "(26800, 5000)\n",
      "(13200, 5000)\n",
      "end  1621403090.18732\n",
      "time:  81.99444079399109\n"
     ]
    }
   ],
   "source": [
    "#KBEST - f_classif\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2,f_classif\n",
    "import time\n",
    "from time import ctime\n",
    "start= time.time()\n",
    "print(\"start \",start)\n",
    "kbest_f_classif = SelectKBest(f_classif, k=5000).fit(train, y_train_raw)\n",
    "X_train_kbest_fclassif = kbest_f_classif.transform(train)\n",
    "X_test_kbest_fclassif = kbest_f_classif.transform(test)\n",
    "print(X_train_kbest_fclassif.shape)\n",
    "print(X_test_kbest_fclassif.shape)\n",
    "end= time.time()\n",
    "print(\"end \",end)\n",
    "print(\"time: \",end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abroad-majority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB\n",
      "START  1621403093.73764\n",
      "END  1621403100.624009\n",
      "GNB Accuracy: 0.5983333333333334 Time: 6.886368989944458  s\n",
      "MNB\n",
      "START  1621403100.624608\n",
      "END  1621403102.08365\n",
      "MNB Accuracy: 0.7119696969696969 Time: 1.4590420722961426  s\n",
      "LinearSVC\n",
      "START  1621403102.083935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END  1621403119.116918\n",
      "LinearSVC Accuracy: 0.7775757575757576 Time: 17.03298306465149  s\n",
      "Decision Tree\n",
      "START  1621403119.117365\n",
      "END  1621403136.802617\n",
      "Decision Tree Accuracy: 0.7318939393939394 Time: 17.68525218963623  s\n",
      "Logistic Regression\n",
      "START  1621403136.8031108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END  1621403170.9354\n",
      "Logistic Regression Accuracy: 0.7925757575757576 Time: 34.13228917121887  s\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from time import ctime\n",
    "\n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          svm.LinearSVC(),\n",
    "          DecisionTreeClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'LinearSVC',\n",
    "          'Decision Tree',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    print(title)\n",
    "    start = time.time()\n",
    "    print(\"START \",start)\n",
    "    model.fit(X_train_kbest_fclassif,y_train_raw)\n",
    "    acc = model.score(X_test_kbest_fclassif,y_test_raw)\n",
    "    end = time.time()\n",
    "    print(\"END \",end)\n",
    "    t = end - start\n",
    "    print(title, \"Accuracy:\",acc, 'Time:', t,\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "conditional-mathematics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  1621403257.132013\n",
      "(26800, 5000)\n",
      "(13200, 5000)\n",
      "end  1621403275.383455\n",
      "time:  18.251441955566406\n"
     ]
    }
   ],
   "source": [
    "# KBEST -chi2\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "import time\n",
    "from time import ctime\n",
    "start= time.time()\n",
    "print(\"start \",start)\n",
    "kbest_chi2 = SelectKBest(chi2, k=5000).fit(train, y_train_raw)\n",
    "X_train_kbest_chi2 = kbest_chi2.transform(train)\n",
    "X_test_kbest_chi2 = kbest_chi2.transform(test)\n",
    "print(X_train_kbest_chi2.shape)\n",
    "print(X_test_kbest_chi2.shape)\n",
    "end= time.time()\n",
    "print(\"end \",end)\n",
    "print(\"time: \",end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vietnamese-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB\n",
      "START  1621403285.7115688\n",
      "END  1621403288.357465\n",
      "MNB Accuracy: 0.7119696969696969 Time: 2.6458961963653564  s\n",
      "LinearSVC\n",
      "START  1621403288.363976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END  1621403304.349289\n",
      "LinearSVC Accuracy: 0.7742424242424243 Time: 15.985312938690186  s\n",
      "Decision Tree\n",
      "START  1621403304.3556879\n",
      "END  1621403321.744792\n",
      "Decision Tree Accuracy: 0.7340909090909091 Time: 17.38910412788391  s\n",
      "Logistic Regression\n",
      "START  1621403321.744971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END  1621403354.9733171\n",
      "Logistic Regression Accuracy: 0.7936363636363636 Time: 33.22834610939026  s\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from time import ctime\n",
    "\n",
    "models = [MultinomialNB(),\n",
    "          svm.LinearSVC(),\n",
    "          DecisionTreeClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['MNB',\n",
    "          'LinearSVC',\n",
    "          'Decision Tree',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    print(title)\n",
    "    start = time.time()\n",
    "    print(\"START \",start)\n",
    "    model.fit(X_train_kbest_chi2,y_train_raw)\n",
    "    acc = model.score(X_test_kbest_chi2,y_test_raw)\n",
    "    end = time.time()\n",
    "    print(\"END \",end)\n",
    "    t = end - start\n",
    "    print(title, \"Accuracy:\",acc, 'Time:', t,\" s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP30027 Machine Learning Assignment 2\n",
    "\n",
    "## Description of text features\n",
    "\n",
    "This notebook describes the pre-computed text features provided for assignment 2. **You do not need to recompute the features yourself for this assignment** -- this information is just for your reference. However, feel free to experiment with different text features if you are interested. If you do want to try generating your own text features, some things to keep in mind:\n",
    "- There are many different decisions you can make throughout the feature design process, from the text preprocessing to the size of the output vectors. There's no guarantee that the defaults we chose will produce the best possible text features for this classification task, so feel free to experiment with different settings.\n",
    "- These features must be trained on a training corpus. Generally, the training corpus should not include validation samples, but for the purposes of this assignment we have used the entire non-test set (training+validation) as the training corpus, to allow you to experiment with different validation sets. If you recompute the text features as part of your own model, you should exclude validation samples and retrain on training samples only. Note that if you do N-fold cross-validation, this means generating N sets of features for N different training-validation splits.\n",
    "- This code may take a long time to run and require a good bit of memory, which is why we are not requiring you to recompute these features yourself. doc2vec in particular is very slow unless you can implement some speed-ups in C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read text\n",
    "# for DEMONSTRATION PURPOSES, the entire training set will be used to train the models and also as a test set\n",
    "x_train_original = pd.read_csv(r\"recipe_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "# use recipe name as an example\n",
    "train_corpus_name = x_train_original['name']\n",
    "test_name = x_train_original['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer\n",
    "\n",
    "A count vectorizer converts documents to vectors which represent word counts. Each column in the output represents a different word and the values indicate the number of times that word appeared in the document. The overall size of a count vector matrix can be quite large (the number of columns is the total number of different words used across all documents in a corpus), but most entries in the matrix are zero (each document contains only a few of all the possible words). Therefore, it is most efficient to represent the count vectors as a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10892\n",
      "(40000, 10892)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# preprocess text and compute counts\n",
    "vocab_name = CountVectorizer(stop_words='english').fit(train_corpus_name)\n",
    "\n",
    "# generate counts for a new set of documents\n",
    "x_train_name = vocab_name.transform(train_corpus_name)\n",
    "x_test_name = vocab_name.transform(test_name)\n",
    "\n",
    "# check the number of words in vocabulary\n",
    "print(len(vocab_name.vocabulary_))\n",
    "# check the shape of sparse matrix\n",
    "print(x_train_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Name\n",
      "Fit and transform\n",
      "Merging\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Test to see approximate accuracy by splitting the training data to train and test data = 7:3\n",
    "\n",
    "data = pd.read_csv(\"recipe_train.csv\")\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(X,y,test_size=0.33)\n",
    "\n",
    "# train_n_steps = pd.DataFrame(X_train_raw.n_steps)\n",
    "# train_n_steps.reset_index(drop=True, inplace=True)\n",
    "# test_n_steps = pd.DataFrame(X_test_raw.n_steps)\n",
    "# test_n_steps.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# train_n_ingredients = pd.DataFrame(X_train_raw.n_ingredients)\n",
    "# train_n_ingredients.reset_index(drop=True, inplace=True)\n",
    "# test_n_ingredients = pd.DataFrame(X_test_raw.n_ingredients)\n",
    "# test_n_ingredients.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(X_train_raw.n_steps)\n",
    "# count vectorizer on feature 'name'\n",
    "print(\"Feature Name\")\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "print(\"Fit and transform\")\n",
    "X_train_name = vec.fit_transform(X_train_raw.name)\n",
    "X_test_name = vec.transform(X_test_raw.name)\n",
    "# put to dataframe\n",
    "df_train_name = pd.DataFrame(X_train_name.todense(),columns = vec.get_feature_names())\n",
    "df_test_name = pd.DataFrame(X_test_name.todense(),columns = vec.get_feature_names())\n",
    "# print(X_train_name.shape, X_test_name.shape)\n",
    "# print(len(X_train_raw.n_steps))\n",
    "\n",
    "# count vectorizer on feature 'steps'\n",
    "X_train_steps = vec.fit_transform(X_train_raw.steps)\n",
    "X_test_steps = vec.transform(X_test_raw.steps)\n",
    "df_train_steps = pd.DataFrame(X_train_steps.todense(),columns = vec.get_feature_names())\n",
    "df_test_steps = pd.DataFrame(X_test_steps.todense(),columns = vec.get_feature_names())\n",
    "# print(df_train_steps)\n",
    "\n",
    "# count vectorizer on feature 'ingredients'\n",
    "# X_train_ing = vec.fit_transform(X_train_raw.ingredients)\n",
    "# X_test_ing = vec.transform(X_test_raw.ingredients)\n",
    "# df_train_ing = pd.DataFrame(X_train_ing.todense(),columns = vec.get_feature_names())\n",
    "# df_test_ing = pd.DataFrame(X_test_ing.todense(),columns = vec.get_feature_names())\n",
    "# print(X_train_ing.shape, X_test_ing.shape)\n",
    "print(\"Merging\")\n",
    "# put all into one dataframe\n",
    "df_train_name_steps =  pd.merge(df_train_name, df_train_steps, how=\"outer\")\n",
    "print(df_train_name_steps)\n",
    "# df_train = pd.merge(df_train_name.head(), df_train_steps.head(), how=\"outer\")\n",
    "# train = pd.concat([df_train_name, df_train_steps,df_train_ing,train_n_steps,train_n_ingredients],axis=1)\n",
    "# test = pd.concat([df_test_name, df_test_steps,df_test_ing,test_n_steps,test_n_ingredients],axis=1)\n",
    "# print(X_train_raw.n_steps)\n",
    "\n",
    "# print(train)\n",
    "# print(X_train_raw.n_steps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB\n",
      "START  1621171833.176519\n",
      "END  1621172001.737783\n",
      "GNB Accuracy: 0.44196969696969696 Time: 168.56126403808594  s\n",
      "MNB\n",
      "START  1621172001.7691991\n",
      "END  1621172036.119162\n",
      "MNB Accuracy: 0.7365909090909091 Time: 34.34996294975281  s\n",
      "LinearSVC\n",
      "START  1621172036.1207008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END  1621172104.9481778\n",
      "LinearSVC Accuracy: 0.7509090909090909 Time: 68.827476978302  s\n",
      "Decision Tree\n",
      "START  1621172104.949993\n",
      "END  1621172165.90699\n",
      "Decision Tree Accuracy: 0.7457575757575757 Time: 60.95699715614319  s\n",
      "Logistic Regression\n",
      "START  1621172165.908127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patriciaangelica/.pyenv/versions/3.7.3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END  1621172880.784724\n",
      "Logistic Regression Accuracy: 0.7946969696969697 Time: 714.8765969276428  s\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from time import ctime\n",
    "models = [GaussianNB(),\n",
    "          MultinomialNB(),\n",
    "          svm.LinearSVC(),\n",
    "          DecisionTreeClassifier(),\n",
    "          LogisticRegression()]\n",
    "titles = ['GNB',\n",
    "          'MNB',\n",
    "          'LinearSVC',\n",
    "          'Decision Tree',\n",
    "          'Logistic Regression']\n",
    "\n",
    "for title, model in zip(titles, models):\n",
    "    print(title)\n",
    "    start = time.time()\n",
    "    print(\"START \",start)\n",
    "    model.fit(train,y_train_raw)\n",
    "    acc = model.score(test,y_test_raw)\n",
    "    end = time.time()\n",
    "    print(\"END \",end)\n",
    "    t = end - start\n",
    "    print(title, \"Accuracy:\",acc, 'Time:', t,\" s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "\n",
    "doc2vec methods are an extension of word2vec. word2vec maps words to a high-dimensional vector space in such a way that words which appear in similar contexts will be close together in the space. doc2vec does a similar embedding for multi-word passages. The doc2vec (or Paragraph Vector) method was introduced by:\n",
    "\n",
    "**Le & Mikolov (2014)** Distributed Representations of Sentences and Documents<br>\n",
    "https://arxiv.org/pdf/1405.4053v2.pdf\n",
    "\n",
    "The implementation of doc2vec used for this project is from gensim and documented here:<br>\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "The size of the output vector is a free parameter. Most implemementations use around 100-300 dimensions, but the best size depends on the problem you're trying to solve with the embeddings and the number of training samples, so you may wish to try different vector sizes. We provided three sets of doc2vec features, and their dimensions are 50 and 100. The vectors themselves represent directions in a high-dimensional concept space; the columns do not represent specific words or phrases. Values in the vector are continuous real numbers and can be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 20)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# size of the output vector\n",
    "vec_size = 20\n",
    "\n",
    "# function to preprocess and tokenize text\n",
    "def tokenize_corpus(txt, tokens_only=False):\n",
    "    for i, line in enumerate(txt):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "# tokenize a training corpus\n",
    "corpus_name = list(tokenize_corpus(train_corpus_name))\n",
    "\n",
    "# train doc2vec on the training corpus\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size, min_count=2, epochs=40)\n",
    "model.build_vocab(corpus_name)\n",
    "model.train(corpus_name, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# tokenize new documents\n",
    "doc = list(tokenize_corpus(test_name, tokens_only=True))\n",
    "\n",
    "# generate embeddings for the new documents\n",
    "x_test_name = np.zeros((len(doc),vec_size))\n",
    "for i in range(len(doc)):\n",
    "    x_test_name[i,:] = model.infer_vector(doc[i])\n",
    "    \n",
    "# check the shape of doc_emb\n",
    "print(x_test_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
